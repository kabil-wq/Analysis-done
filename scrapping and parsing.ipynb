{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58c43e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path =(r\"C:\\Users\\kabil\\Downloads\\Input.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b174381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved article bctech2011\n",
      "Extracted and saved article bctech2012\n",
      "Extracted and saved article bctech2013\n",
      "Extracted and saved article bctech2014\n",
      "Extracted and saved article bctech2015\n",
      "Extracted and saved article bctech2016\n",
      "Extracted and saved article bctech2017\n",
      "Extracted and saved article bctech2018\n",
      "Extracted and saved article bctech2019\n",
      "Extracted and saved article bctech2020\n",
      "Extracted and saved article bctech2021\n",
      "Extracted and saved article bctech2022\n",
      "Extracted and saved article bctech2023\n",
      "Extracted and saved article bctech2024\n",
      "Extracted and saved article bctech2025\n",
      "Extracted and saved article bctech2026\n",
      "Extracted and saved article bctech2027\n",
      "Extracted and saved article bctech2028\n",
      "Extracted and saved article bctech2029\n",
      "Extracted and saved article bctech2030\n",
      "Extracted and saved article bctech2031\n",
      "Extracted and saved article bctech2032\n",
      "Extracted and saved article bctech2033\n",
      "Extracted and saved article bctech2034\n",
      "Extracted and saved article bctech2035\n",
      "Extracted and saved article bctech2036\n",
      "Extracted and saved article bctech2037\n",
      "Extracted and saved article bctech2038\n",
      "Extracted and saved article bctech2039\n",
      "Extracted and saved article bctech2040\n",
      "Extracted and saved article bctech2041\n",
      "Extracted and saved article bctech2042\n",
      "Extracted and saved article bctech2043\n",
      "Extracted and saved article bctech2044\n",
      "Extracted and saved article bctech2045\n",
      "Extracted and saved article bctech2046\n",
      "Extracted and saved article bctech2047\n",
      "Extracted and saved article bctech2048\n",
      "Extracted and saved article bctech2049\n",
      "Extracted and saved article bctech2050\n",
      "Extracted and saved article bctech2051\n",
      "Extracted and saved article bctech2052\n",
      "Extracted and saved article bctech2053\n",
      "Extracted and saved article bctech2054\n",
      "Extracted and saved article bctech2055\n",
      "Extracted and saved article bctech2056\n",
      "Extracted and saved article bctech2057\n",
      "Extracted and saved article bctech2058\n",
      "Extracted and saved article bctech2059\n",
      "Extracted and saved article bctech2060\n",
      "Extracted and saved article bctech2061\n",
      "Extracted and saved article bctech2062\n",
      "Extracted and saved article bctech2063\n",
      "Extracted and saved article bctech2064\n",
      "Extracted and saved article bctech2065\n",
      "Extracted and saved article bctech2066\n",
      "Extracted and saved article bctech2067\n",
      "Extracted and saved article bctech2068\n",
      "Extracted and saved article bctech2069\n",
      "Extracted and saved article bctech2070\n",
      "Extracted and saved article bctech2071\n",
      "Extracted and saved article bctech2072\n",
      "Extracted and saved article bctech2073\n",
      "Extracted and saved article bctech2074\n",
      "Extracted and saved article bctech2075\n",
      "Extracted and saved article bctech2076\n",
      "Extracted and saved article bctech2077\n",
      "Extracted and saved article bctech2078\n",
      "Extracted and saved article bctech2079\n",
      "Extracted and saved article bctech2080\n",
      "Extracted and saved article bctech2081\n",
      "Extracted and saved article bctech2082\n",
      "Extracted and saved article bctech2083\n",
      "Extracted and saved article bctech2084\n",
      "Extracted and saved article bctech2085\n",
      "Extracted and saved article bctech2086\n",
      "Extracted and saved article bctech2087\n",
      "Extracted and saved article bctech2088\n",
      "Extracted and saved article bctech2089\n",
      "Extracted and saved article bctech2090\n",
      "Extracted and saved article bctech2091\n",
      "Extracted and saved article bctech2092\n",
      "Extracted and saved article bctech2093\n",
      "Extracted and saved article bctech2094\n",
      "Extracted and saved article bctech2095\n",
      "Extracted and saved article bctech2096\n",
      "Extracted and saved article bctech2097\n",
      "Extracted and saved article bctech2098\n",
      "Extracted and saved article bctech2099\n",
      "Extracted and saved article bctech2100\n",
      "Extracted and saved article bctech2101\n",
      "Extracted and saved article bctech2102\n",
      "Extracted and saved article bctech2103\n",
      "Extracted and saved article bctech2104\n",
      "Extracted and saved article bctech2105\n",
      "Extracted and saved article bctech2106\n",
      "Extracted and saved article bctech2107\n",
      "Extracted and saved article bctech2108\n",
      "Extracted and saved article bctech2109\n",
      "Extracted and saved article bctech2110\n",
      "Extracted and saved article bctech2111\n",
      "Extracted and saved article bctech2112\n",
      "Extracted and saved article bctech2113\n",
      "Extracted and saved article bctech2114\n",
      "Extracted and saved article bctech2115\n",
      "Extracted and saved article bctech2116\n",
      "Extracted and saved article bctech2117\n",
      "Extracted and saved article bctech2118\n",
      "Extracted and saved article bctech2119\n",
      "Extracted and saved article bctech2120\n",
      "Extracted and saved article bctech2121\n",
      "Extracted and saved article bctech2122\n",
      "Extracted and saved article bctech2123\n",
      "Extracted and saved article bctech2124\n",
      "Extracted and saved article bctech2125\n",
      "Extracted and saved article bctech2126\n",
      "Extracted and saved article bctech2127\n",
      "Extracted and saved article bctech2128\n",
      "Extracted and saved article bctech2129\n",
      "Extracted and saved article bctech2130\n",
      "Extracted and saved article bctech2131\n",
      "Extracted and saved article bctech2132\n",
      "Extracted and saved article bctech2133\n",
      "Extracted and saved article bctech2134\n",
      "Extracted and saved article bctech2135\n",
      "Extracted and saved article bctech2136\n",
      "Extracted and saved article bctech2137\n",
      "Extracted and saved article bctech2138\n",
      "Extracted and saved article bctech2139\n",
      "Extracted and saved article bctech2140\n",
      "Extracted and saved article bctech2141\n",
      "Extracted and saved article bctech2142\n",
      "Extracted and saved article bctech2143\n",
      "Extracted and saved article bctech2144\n",
      "Extracted and saved article bctech2145\n",
      "Extracted and saved article bctech2146\n",
      "Extracted and saved article bctech2147\n",
      "Extracted and saved article bctech2148\n",
      "Extracted and saved article bctech2149\n",
      "Extracted and saved article bctech2150\n",
      "Extracted and saved article bctech2151\n",
      "Extracted and saved article bctech2152\n",
      "Extracted and saved article bctech2153\n",
      "Extracted and saved article bctech2154\n",
      "Extracted and saved article bctech2155\n",
      "Extracted and saved article bctech2156\n",
      "Extracted and saved article bctech2157\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Define a directory to save the extracted articles\n",
    "output_dir = 'extracted_articles'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract the title and main article text from a URL\n",
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the title\n",
    "    title = soup.find('title').get_text(strip=True)\n",
    "    \n",
    "    # Extract the article text (assuming it's within specific tags like <p> or <div>)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    article_text = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
    "    \n",
    "    return title, article_text\n",
    "\n",
    "# Load the Excel fil\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Iterate over each URL and extract the text\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    try:\n",
    "        title, article_text = extract_article_text(url)\n",
    "        \n",
    "        # Save the extracted text into a file\n",
    "        file_path = os.path.join(output_dir, f'{url_id}.txt')\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {title}\\n\\n{article_text}\")\n",
    "        \n",
    "        print(f\"Extracted and saved article {url_id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract article {url_id}: {e}\")\n",
    "\n",
    "print(\"Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b66d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblobNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "     ------------------------------------ 626.3/626.3 kB 172.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk in c:\\users\\kabil\\anaconda3\\lib\\site-packages (3.7)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 589.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\kabil\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kabil\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kabil\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\kabil\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kabil\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "pip install textblob nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381054db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kabil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kabil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d9b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to C:\\Users\\kabil\\Downloads\\Output Data Structure (1).xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "stop_words_path = 'C:\\\\Users\\\\kabil\\\\Downloads\\\\StopWords\\\\stopwords.txt'\n",
    "positive_dict_path = r\"C:\\Users\\kabil\\Downloads\\MasterDictionary\\positive-words.txt\"\n",
    "negative_dict_path = r\"C:\\Users\\kabil\\Downloads\\MasterDictionary\\negative-words.txt\"\n",
    "input_dir = 'extracted_articles'  # relative path\n",
    "output_file = 'C:\\\\Users\\\\kabil\\\\Downloads\\\\Output Data Structure (1).xlsx'\n",
    "\n",
    "# Load stop words and positive/negative dictionaries\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(positive_dict_path) as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "\n",
    "with open(negative_dict_path) as f:\n",
    "    negative_words = set(f.read().splitlines())\n",
    "\n",
    "# Define function to compute text metrics\n",
    "def compute_metrics(text):\n",
    "    # Tokenize text and remove non-alphabetic tokens\n",
    "    words = [word for word in word_tokenize(text.lower()) if word.isalpha()]\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Filter out stop words\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
    "    total_words = len(filtered_words)\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    # Calculate readability metrics\n",
    "    complex_words = [word for word in filtered_words if len(re.findall(r'[aeiou]', word)) > 2]\n",
    "    avg_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "    percentage_complex_words = len(complex_words) / total_words if total_words else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = total_words / total_sentences if total_sentences else 0\n",
    "    syllable_count_per_word = np.mean([len(re.findall(r'[aeiou]', word)) for word in filtered_words]) if total_words else 0\n",
    "    personal_pronouns = len(re.findall(r'\\b(i|we|my|ours|us)\\b', text, re.IGNORECASE))\n",
    "    avg_word_length = np.mean([len(word) for word in filtered_words]) if total_words else 0\n",
    "\n",
    "    # Create output dictionary\n",
    "    metrics = {\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity,\n",
    "        'Subjectivity Score': subjectivity,\n",
    "        'Avg Sentence Length': avg_sentence_length,\n",
    "        'Percentage of Complex Words': percentage_complex_words,\n",
    "        'Fog Index': fog_index,\n",
    "        'Avg Number of Words Per Sentence': avg_words_per_sentence,\n",
    "        'Complex Word Count': len(complex_words),\n",
    "        'Word Count': total_words,\n",
    "        'Syllable Per Word': syllable_count_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Avg Word Length': avg_word_length\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Prepare DataFrame for results\n",
    "columns = [\n",
    "    'URL_ID', 'Positive Score', 'Negative Score', 'Polarity Score',\n",
    "    'Subjectivity Score', 'Avg Sentence Length', 'Percentage of Complex Words',\n",
    "    'Fog Index', 'Avg Number of Words Per Sentence', 'Complex Word Count',\n",
    "    'Word Count', 'Syllable Per Word', 'Personal Pronouns', 'Avg Word Length'\n",
    "]\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# List to hold individual dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Process each article\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        url_id = file_name.replace('.txt', '')\n",
    "        with open(os.path.join(input_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            metrics = compute_metrics(text)\n",
    "            metrics['URL_ID'] = url_id\n",
    "            # Convert metrics to DataFrame and append to list\n",
    "            metrics_df = pd.DataFrame([metrics], columns=columns)\n",
    "            dataframes.append(metrics_df)\n",
    "\n",
    "# Concatenate all individual dataframes into one\n",
    "results_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save results to Excel\n",
    "results_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Analysis complete. Results saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8374be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated analysis complete. Results saved to C:\\Users\\kabil\\Downloads\\Updated_Output_Data_Structure.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "input_file = r\"C:\\Users\\kabil\\Downloads\\Input.xlsx\"\n",
    "output_file = 'C:\\\\Users\\\\kabil\\\\Downloads\\\\Output Data Structure (1).xlsx'\n",
    "\n",
    "# Load the input data containing URL_ID and URL\n",
    "input_df = pd.read_excel(input_file, usecols=['URL_ID', 'URL'])\n",
    "\n",
    "# Create a dictionary mapping URL_ID to URL\n",
    "url_mapping = dict(zip(input_df['URL_ID'], input_df['URL']))\n",
    "\n",
    "# Load the results DataFrame\n",
    "results_df = pd.read_excel(output_file)\n",
    "\n",
    "# Add the URL column to the results DataFrame\n",
    "results_df['URL'] = results_df['URL_ID'].map(url_mapping)\n",
    "\n",
    "# Define the columns to match the desired order\n",
    "columns = [\n",
    "    'URL_ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score',\n",
    "    'Subjectivity Score', 'Avg Sentence Length', 'Percentage of Complex Words',\n",
    "    'Fog Index', 'Avg Number of Words Per Sentence', 'Complex Word Count',\n",
    "    'Word Count', 'Syllable Per Word', 'Personal Pronouns', 'Avg Word Length'\n",
    "]\n",
    "\n",
    "# Reorder columns to match the desired order\n",
    "results_df = results_df[columns]\n",
    "\n",
    "# Save the updated results with the URL column to a new Excel file\n",
    "updated_output_file = 'C:\\\\Users\\\\kabil\\\\Downloads\\\\Updated_Output_Data_Structure.xlsx'\n",
    "results_df.to_excel(updated_output_file, index=False)\n",
    "\n",
    "print(\"Updated analysis complete. Results saved to\", updated_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9ef2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
